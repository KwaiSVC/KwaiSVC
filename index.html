<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<!-- saved from url=(0043)http://www-personal.umich.edu/~ywchao/hico/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<style>
body {
    background-color: f5f5f5;
}
</style>

<title>Search-oriented Micro-video Captioning</title>
    <link rel="stylesheet" href="assets/css/styles.css" />
</head>

<body>

<table border="0" width="600px" align="center">
<tbody><tr>
<td>

<!-- Title -->
<!-- <center>
<h1>
<font face="helvetica" style="font-size:87%">
    Search-oriented Micro-video Captioning
</font>
</h1>
</center> -->

<br>

<!-- Figure -->
<center>
<img src="./CSAN/figs/kwaiSVC-BG.jpg" height="300">
</center>

<!-- Abstract -->
<center>
<h2><font face="helvetica" style="font-size:24px">Abstract</font></h2>
<hr style="margin-top:-10px; margin-bottom:13px">
</center>


<font face="helvetica" style="font-size:15px">
<div style= "text-indent:25px; line-height: 1.5; text-align:justify">
    Pioneer efforts have been dedicated to the content-oriented video
captioning that generates relevant sentences to describe the visual
contents of a given video from the producer perspective. By contrast,
this work targets at the search-oriented one that summarizes
the given video via generating query-like sentences from the consumer
angle. Beyond relevance, diversity is vital in characterizing
consumers’ seeking intention from different aspects. Towards this
end, we devise a large-scale multimodal pre-training network regularized
by five tasks to strengthen the downstream video representation,
which is well-trained over our collected 11M micro-videos.
Thereafter, we present a flow-based diverse captioning model to
generate different captions from consumers’ search demand. This
model is optimized via a reconstruction loss and a KL divergence
between the prior and the posterior. We justify our model over our
constructed golden dataset comprising 690k &ltquery, micro-video&gt
pairs and experimental results demonstrate its superiority.
</div>

<!-- Code -->
<center>
<h2><font face="helvetica" style="font-size:24px">Code</font></h2>
<hr style="margin-top:-10px; margin-bottom:13px">
</center>

<center>
    <style type="text/css">
    .tg  {border-collapse:collapse;border-spacing:0;}
    .tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
      overflow:hidden;padding:10px 10px;word-break:normal;}
    .tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
      font-weight:normal;overflow:hidden;padding:10px 10px;word-break:normal;}
    .tg .tg-baqh{text-align:center;vertical-align:top}
    .tg .tg-wa1i{font-weight:bold;text-align:center;vertical-align:middle}
    .tg .tg-amwm{font-weight:bold;text-align:center;vertical-align:top}
    .tg .tg-nrix{text-align:center;vertical-align:middle}
    </style>
    <table class="tg">
    <thead>
      <tr>
        <th class="tg-amwm" rowspan="2"><span style="color:#000">Model</span></th>
        <th class="tg-amwm" colspan="2"><span style="color:#000">Diversity</span></th>
        <th class="tg-amwm" colspan="6"><span style="color:#000">Relevance</span></th>
        <th class="tg-amwm" rowspan="2"><span style="color:#000">R/D</span></th>
      </tr>
      <tr>
        <th class="tg-amwm"><span style="color:#000">mB4</span></th>
        <th class="tg-amwm"><span style="color:#000">U</span></th>
        <th class="tg-amwm"><span style="color:#000">B1</span></th>
        <th class="tg-amwm"><span style="color:#000">B2</span></th>
        <th class="tg-amwm"><span style="color:#000">B3</span></th>
        <th class="tg-amwm"><span style="color:#000">B4</span></th>
        <th class="tg-amwm"><span style="color:#000">R</span></th>
        <th class="tg-amwm"><span style="color:#000">C</span></th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td class="tg-baqh"><span style="color:#000">CVAE</span></td>
        <td class="tg-nrix">0.917</td>
        <td class="tg-nrix">3.83%</td>
        <td class="tg-nrix">0.861</td>
        <td class="tg-nrix">0.822</td>
        <td class="tg-nrix">0.781</td>
        <td class="tg-nrix">0.747</td>
        <td class="tg-nrix">0.811</td>
        <td class="tg-nrix">2.95</td>
        <td class="tg-nrix">0.815</td>
      </tr>
      <tr>
        <td class="tg-baqh"><span style="color:#000">AG-CVAE</span></td>
        <td class="tg-nrix">0.845</td>
        <td class="tg-nrix">8.70%</td>
        <td class="tg-nrix">0.86</td>
        <td class="tg-nrix">0.822</td>
        <td class="tg-nrix">0.781</td>
        <td class="tg-nrix">0.745</td>
        <td class="tg-nrix">0.818</td>
        <td class="tg-nrix">2.95</td>
        <td class="tg-nrix">0.882</td>
      </tr>
      <tr>
        <td class="tg-baqh"><span style="color:#000">DCM</span></td>
        <td class="tg-nrix">0.437</td>
        <td class="tg-nrix">73.50%</td>
        <td class="tg-nrix">0.666</td>
        <td class="tg-nrix">0.555</td>
        <td class="tg-nrix">0.457</td>
        <td class="tg-nrix">0.378</td>
        <td class="tg-nrix">0.606</td>
        <td class="tg-nrix">1.71</td>
        <td class="tg-nrix">0.865</td>
      </tr>
      <tr>
        <td class="tg-baqh"><span style="color:#000">POS</span></td>
        <td class="tg-nrix">0.953</td>
        <td class="tg-nrix">2.33%</td>
        <td class="tg-nrix">0.855</td>
        <td class="tg-nrix">0.816</td>
        <td class="tg-nrix">0.773</td>
        <td class="tg-nrix">0.738</td>
        <td class="tg-nrix">0.804</td>
        <td class="tg-nrix">2.94</td>
        <td class="tg-nrix">0.774</td>
      </tr>
      <tr>
        <td class="tg-baqh"><span style="color:#000">Seq-CVAE</span></td>
        <td class="tg-nrix">0.78</td>
        <td class="tg-nrix">16.40%</td>
        <td class="tg-nrix">0.845</td>
        <td class="tg-nrix">0.803</td>
        <td class="tg-nrix">0.757</td>
        <td class="tg-nrix">0.719</td>
        <td class="tg-nrix">0.786</td>
        <td class="tg-nrix">2.88</td>
        <td class="tg-nrix">0.922</td>
      </tr>
      <tr>
        <td class="tg-amwm"><span style="color:#000">FLIP (Ours)</span></td>
        <td class="tg-wa1i">0.692</td>
        <td class="tg-wa1i">23.20%</td>
        <td class="tg-wa1i">0.854</td>
        <td class="tg-wa1i">0.813</td>
        <td class="tg-wa1i">0.77</td>
        <td class="tg-wa1i">0.733</td>
        <td class="tg-wa1i">0.8</td>
        <td class="tg-wa1i">2.89</td>
        <td class="tg-wa1i">1.059</td>
      </tr>
    </tbody>
    </table>
</center>


<br>

<font face="helvetica" style="font-size:15px">
<div>    
    <a href="" style="text-decoration: none;">
    <img src="./CSAN/figs/download_button.jpg" height="30px">
    </a>
    <div style="margin-left: 10px; margin-top: 1px; display: inline-block;">
        <a href="https://pan.baidu.com/s/1S7WBzPlmpYpdQC0t9zhQtw?pwd=v5r4">Baidu Cloud</a> (password: v5r4)
        <br>
        Code & checkpoints: pretraining (MEEK), diverse captioning (FLIP), and baseline models.
    </div>
</div>

<br>

<!-- Dataset -->
<center>
<h2><font face="helvetica" style="font-size:24px">Dataset</font></h2>
<hr style="margin-top:-10px; margin-bottom:13px">
</center>

    <!-- KwaiSVC-222k -->
<font face="helvetica" style="font-size:15px">
<div>
    <h2><font face="helvetica" style="font-size:20px">KwaiSVC-222k</font></h2>

    <div style= "text-indent:25px; line-height: 1.5; text-align:justify">
        KwaiSVC-222k is a golden dataset for search-oriented micro-video captioning. 
It is based on the users' video search behavior in Kuaishou micro-video platform. 
Specifically, we filter search logs about query-click behavior to 
get high quality &ltquery, micro-video&gt pairs. The filter rules
consist of video view count, click through rate, and play completion rate.
    </div>
    <br>
    <a href="" style="text-decoration: none;">
    <img src="./CSAN/figs/download_button.jpg" height="30px">
    </a>
    <div style="margin-left: 10px; margin-top: 1px; display: inline-block;">
        <a href="https://pan.baidu.com/s/1v6x14o5K9IuM3A-IS29UoA?pwd=ihc2">Baidu Cloud</a> (password: ihc2)
        <br>
        KwaiSVC-222k dataset.
        <!-- It contains video-query pairs, extracted video and text features, and checkpoints of MEEK and FLIP. -->
    </div>
</div>

<br>

    <!-- KwaiSVC-11M -->
<div>
    <h2><font face="helvetica" style="font-size:20px">KwaiSVC-11M </font></h2>

    <div style= "text-indent:25px; line-height: 1.5; text-align:justify">
        KwaiSVC-11M is a large multimodal pretraining dataset collected for solving the multimodal representation learning challenge.
Based on this dataset, we devise a large-scale Multimodal prE-training nEtwork (MEEK), which improves the caption performance.
This dataset is constructed similarly to KwaiSVC-222k. The only difference is that we relax the filter rules to get more data.
    </div>
    <div style= "text-indent:25px; line-height: 1.5; text-align:justify">
        Due to copyright issues, this dataset is only provided by request.
    </div>
</div>

<br>

<!-- Last Update
<hr>
<font face="helvetica" style="font-size:15px">Last updated on 10/6/2022</font>
</td>
</tr>
-->

</tbody></table></body></html>


