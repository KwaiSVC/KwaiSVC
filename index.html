<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<!-- saved from url=(0043)http://www-personal.umich.edu/~ywchao/hico/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<style>
body {
    background-color: f5f5f5;
}
</style>

<title>Search-oriented Micro-video Captioning</title>
    <link rel="stylesheet" href="assets/css/styles.css" />
</head>

<body>

<table border="0" width="600px" align="center">
<tbody><tr>
<td>

<!-- Title -->
<!-- <center>
<h1>
<font face="helvetica" style="font-size:87%">
    Search-oriented Micro-video Captioning
</font>
</h1>
</center> -->

<br>

<!-- Figure -->
<center>
<img src="./CSAN/figs/kwaiSVC-BG.jpg" height="300">
</center>

<!-- Abstract -->
<center>
<h2><font face="helvetica" style="font-size:24px">Abstract</font></h2>
<hr style="margin-top:-10px; margin-bottom:13px">
</center>


<font face="helvetica" style="font-size:15px">
<div style= "text-indent:25px; line-height: 1.5; text-align:justify">
    Pioneer efforts have been dedicated to the content-oriented video
captioning that generates relevant sentences to describe the visual
contents of a given video from the producer perspective. By contrast,
this work targets at the search-oriented one that summarizes
the given video via generating query-like sentences from the consumer
angle. Beyond relevance, diversity is vital in characterizing
consumers’ seeking intention from different aspects. Towards this
end, we devise a large-scale multimodal pre-training network regularized
by five tasks to strengthen the downstream video representation,
which is well-trained over our collected 11M micro-videos.
Thereafter, we present a flow-based diverse captioning model to
generate different captions from consumers’ search demand. This
model is optimized via a reconstruction loss and a KL divergence
between the prior and the posterior. We justify our model over our
constructed golden dataset comprising 690k &ltquery, micro-video&gt
pairs and experimental results demonstrate its superiority.
</div>


<!-- Dataset -->
<br>
<center>
<h2><font face="helvetica" style="font-size:24px">Datasets</font></h2>
<hr style="margin-top:-10px; margin-bottom:13px">
</center>

<center>
    <img src="./CSAN/figs/two_datasets.png" height="300">
</center>

    <!-- KwaiSVC-222k -->
<font face="helvetica" style="font-size:15px">
<div>
    <h2><font face="helvetica" style="font-size:20px">KwaiSVC-222k</font></h2>

    <div style= "text-indent:25px; line-height: 1.5; text-align:justify">
        KwaiSVC-222k is a golden dataset for search-oriented micro-video captioning. 
It is based on the users' video search behavior in Kuaishou micro-video platform. 
Specifically, we filter search logs about query-click behavior to 
get high quality &ltquery, micro-video&gt pairs. The filter rules
consist of video view count, click through rate, and play completion rate.
    </div>
    <br>
    <a href="" style="text-decoration: none;">
    <img src="./CSAN/figs/download_button.jpg" height="30px">
    </a>
    <div style="margin-left: 10px; margin-top: 1px; display: inline-block;">
        <a href="https://pan.baidu.com/s/1v6x14o5K9IuM3A-IS29UoA?pwd=ihc2">Baidu Cloud</a> (password: ihc2)
        <br>
        KwaiSVC-222k dataset.
        <!-- It contains video-query pairs, extracted video and text features, and checkpoints of MEEK and FLIP. -->
    </div>
</div>

    <!-- KwaiSVC-11M -->
<div>
    <h2><font face="helvetica" style="font-size:20px">KwaiSVC-11M </font></h2>

    <div style= "text-indent:25px; line-height: 1.5; text-align:justify">
        KwaiSVC-11M is a large multimodal pretraining dataset collected for solving the multimodal representation learning challenge.
Based on this dataset, we devise a large-scale Multimodal prE-training nEtwork (MEEK), which improves the caption performance.
This dataset is constructed similarly to KwaiSVC-222k. The only difference is that we relax the filter rules to get more data.
    </div>
    <br>
    <a href="" style="text-decoration: none;">
    <img src="./CSAN/figs/download_button.jpg" height="30px" style="vertical-align:middle;">
    </a>
    <div style="margin-left: 10px; margin-top: 1px; display: inline-block;">
        Due to copyright issues, this dataset is only provided by <a href="mailto:nieliqiang@gmail.com">request</a>.
    </div>
</div>

    <!-- Dataset statistics -->
<h2><font face="helvetica" style="font-size:20px">Dataset statistics </font></h2>

<center>
<style type="text/css">
    .tg  {border-collapse:collapse;border-spacing:0;}
    .tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
      overflow:hidden;padding:10px 5px;word-break:normal;}
    .tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
      font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
    .tg .tg-z27t{background-color:#154D9D;text-align:center;vertical-align:middle}
    .tg .tg-f4yw{background-color:#FFF;text-align:center;vertical-align:middle}
    </style>
    <table class="tg">
    <caption style="font-size: 18px"><b> Comparison among existing video captioning datasets </b></caption>
    <thead>
      <tr>
        <th class="tg-z27t"><span style="font-weight:bold;color:white">Dataset</span></th>
        <th class="tg-z27t"><span style="font-weight:bold;color:white">Clips</span></th>
        <th class="tg-z27t"><span style="font-weight:bold;color:white">Captions</span></th>
        <th class="tg-z27t"><span style="font-weight:bold;color:white">Pairs</span></th>
        <th class="tg-z27t"><span style="font-weight:bold;color:white">Purpose</span></th>
        <th class="tg-z27t"><span style="font-weight:bold;color:white">Category</span></th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td class="tg-f4yw"><span style="color:black">MSVD</span></td>
        <td class="tg-f4yw"><span style="color:black">2k</span></td>
        <td class="tg-f4yw"><span style="color:black">80k</span></td>
        <td class="tg-f4yw"><span style="color:black">80k</span></td>
        <td class="tg-f4yw"><span style="color:black">content-oriented</span></td>
        <td class="tg-f4yw"><span style="color:black">-</span></td>
      </tr>
      <tr>
        <td class="tg-f4yw"><span style="color:black">MSR-VTT</span></td>
        <td class="tg-f4yw"><span style="color:black">10k</span></td>
        <td class="tg-f4yw"><span style="color:black">200k</span></td>
        <td class="tg-f4yw"><span style="color:black">200k</span></td>
        <td class="tg-f4yw"><span style="color:black">content-oriented</span></td>
        <td class="tg-f4yw"><span style="color:black">20</span></td>
      </tr>
      <tr>
        <td class="tg-f4yw"><span style="color:black">Kwai-SVC-222k</span></td>
        <td class="tg-f4yw"><span style="color:black">222k</span></td>
        <td class="tg-f4yw"><span style="color:black">144k</span></td>
        <td class="tg-f4yw"><span style="color:black">690k</span></td>
        <td class="tg-f4yw"><span style="color:black">search-oriented</span></td>
        <td class="tg-f4yw"><span style="color:black">32</span></td>
      </tr>
      <tr>
        <td class="tg-f4yw"><span style="color:black">Kwai-SVC-11M</span></td>
        <td class="tg-f4yw"><span style="color:black">11M</span></td>
        <td class="tg-f4yw"><span style="color:black">4M</span></td>
        <td class="tg-f4yw"><span style="color:black">35M</span></td>
        <td class="tg-f4yw"><span style="color:black">search-oriented</span></td>
        <td class="tg-f4yw"><span style="color:black">35</span></td>
      </tr>
    </tbody>
    </table>
</center>

<br>
<center>
    <img src="./CSAN/figs/cat_statistics.png" height="500">
</center>

<!-- Code -->
<br>
<center>
    <h2><font face="helvetica" style="font-size:24px">Code</font></h2>
    <hr style="margin-top:-10px; margin-bottom:13px">
</center>
    
    <center>
    <style type="text/css">
    .tg  {border-collapse:collapse;border-spacing:0;}
    .tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
      overflow:hidden;padding:10px 10px;word-break:normal;}
    .tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
      font-weight:normal;overflow:hidden;padding:10px 10px;word-break:normal;}
    .tg .tg-4337{background-color:#154d9d;color:#ffffff;text-align:center;vertical-align:top}
    .tg .tg-baqh{text-align:center;vertical-align:top}
    .tg .tg-wa1i{font-weight:bold;text-align:center;vertical-align:middle}
    .tg .tg-aege{background-color:#154D9D;color:#ffffff;font-weight:bold;text-align:center;vertical-align:middle}
    .tg .tg-w4by{background-color:#154d9d;color:#ffffff;font-weight:bold;text-align:center;vertical-align:middle}
    .tg .tg-7yig{background-color:#FFF;text-align:center;vertical-align:top}
    .tg .tg-f4yw{background-color:#FFF;text-align:center;vertical-align:middle}
    .tg .tg-nrix{text-align:center;vertical-align:middle}
    .tg .tg-amwm{font-weight:bold;text-align:center;vertical-align:top}
    </style>
    <table class="tg">
    <caption style="font-size:18px"> <b> Performance Comparison for Search-oriented Micro-video Captioning </b></caption>
    <thead>
      <tr>
        <th class="tg-aege" rowspan="2">Model</th>
        <th class="tg-w4by" colspan="2">Diversity</th>
        <th class="tg-w4by" colspan="6">Relevance</th>
        <th class="tg-w4by" rowspan="2">R/D</th>
      </tr>
      <tr>
        <th class="tg-w4by">mB4</th>
        <th class="tg-w4by">U</th>
        <th class="tg-w4by">B1</th>
        <th class="tg-w4by">B2</th>
        <th class="tg-w4by">B3</th>
        <th class="tg-w4by">B4</th>
        <th class="tg-w4by">R</th>
        <th class="tg-w4by">C</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td class="tg-7yig"><span style="color:#000"><a href="https://arxiv.org/pdf/1703.10960.pdf">CVAE</a></span></td>
        <td class="tg-f4yw">0.917</td>
        <td class="tg-f4yw">3.83%</td>
        <td class="tg-f4yw">0.861</td>
        <td class="tg-f4yw">0.822</td>
        <td class="tg-f4yw">0.781</td>
        <td class="tg-nrix">0.747</td>
        <td class="tg-nrix">0.811</td>
        <td class="tg-nrix">2.950</td>
        <td class="tg-nrix">0.815</td>
      </tr>
      <tr>
        <td class="tg-7yig"><span style="color:#000"><a href="https://proceedings.neurips.cc/paper/2017/file/4b21cf96d4cf612f239a6c322b10c8fe-Paper.pdf">AG-CVAE></span></td>
        <td class="tg-f4yw">0.845</td>
        <td class="tg-f4yw">8.70%</td>
        <td class="tg-f4yw">0.860</td>
        <td class="tg-f4yw">0.822</td>
        <td class="tg-f4yw">0.781</td>
        <td class="tg-nrix">0.745</td>
        <td class="tg-nrix">0.818</td>
        <td class="tg-nrix">2.950</td>
        <td class="tg-nrix">0.882</td>
      </tr>
      <tr>
        <td class="tg-7yig"><span style="color:#000"><a href="https://arxiv.org/pdf/1910.12019.pdf">DCM</a></span></td>
        <td class="tg-f4yw">0.437</td>
        <td class="tg-f4yw">73.50%</td>
        <td class="tg-f4yw">0.666</td>
        <td class="tg-f4yw">0.555</td>
        <td class="tg-f4yw">0.457</td>
        <td class="tg-nrix">0.378</td>
        <td class="tg-nrix">0.606</td>
        <td class="tg-nrix">1.710</td>
        <td class="tg-nrix">0.865</td>
      </tr>
      <tr>
        <td class="tg-baqh"><span style="color:#000"><a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Deshpande_Fast_Diverse_and_Accurate_Image_Captioning_Guided_by_Part-Of-Speech_CVPR_2019_paper.pdf">POS</a></span></td>
        <td class="tg-nrix">0.953</td>
        <td class="tg-nrix">2.33%</td>
        <td class="tg-nrix">0.855</td>
        <td class="tg-nrix">0.816</td>
        <td class="tg-nrix">0.773</td>
        <td class="tg-nrix">0.738</td>
        <td class="tg-nrix">0.804</td>
        <td class="tg-nrix">2.940</td>
        <td class="tg-nrix">0.774</td>
      </tr>
      <tr>
        <td class="tg-baqh"><span style="color:#000"><a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Aneja_Sequential_Latent_Spaces_for_Modeling_the_Intention_During_Diverse_Image_ICCV_2019_paper.pdf">Seq-CVAE</a></span></td>
        <td class="tg-nrix">0.780</td>
        <td class="tg-nrix">16.40%</td>
        <td class="tg-nrix">0.845</td>
        <td class="tg-nrix">0.803</td>
        <td class="tg-nrix">0.757</td>
        <td class="tg-nrix">0.719</td>
        <td class="tg-nrix">0.786</td>
        <td class="tg-nrix">2.880</td>
        <td class="tg-nrix">0.922</td>
      </tr>
      <tr>
        <td class="tg-amwm"><span style="color:#000"><a href="https://dl.acm.org/doi/10.1145/3503161.3548180">FLIP (Ours)</a></span></td>
        <td class="tg-wa1i">0.692</td>
        <td class="tg-wa1i">23.20%</td>
        <td class="tg-nrix">0.854</td>
        <td class="tg-nrix">0.813</td>
        <td class="tg-nrix">0.770</td>
        <td class="tg-nrix">0.733</td>
        <td class="tg-nrix">0.800</td>
        <td class="tg-nrix">2.890</td>
        <td class="tg-wa1i">1.059</td>
      </tr>
    </tbody>
    </table>
    </center>
    
    
    <br>
    
    <font face="helvetica" style="font-size:15px">
    <div>    
        <a href="" style="text-decoration: none;">
        <img src="./CSAN/figs/github.png" height="30px">
        </a>
        <div style="margin-left: 10px; margin-top: 1px; display: inline-block;">
            <a href="https://pan.baidu.com/s/1S7WBzPlmpYpdQC0t9zhQtw?pwd=v5r4">Baidu Cloud</a> (password: v5r4)
            <br>
            Code & checkpoints: pretraining (MEEK), diverse captioning (FLIP), and baseline models.
        </div>
    </div>

<!-- Last Update
<hr>
<font face="helvetica" style="font-size:15px">Last updated on 10/6/2022</font>
</td>
</tr>
-->

<!-- Paper -->
<br>
<center>
    <h2><font face="helvetica" style="font-size:24px">Paper</font></h2>
    <hr style="margin-top:-10px; margin-bottom:13px">
</center>

<img src="./CSAN/figs/papers.jpg" height="40px" >
</a>
<div style="margin-left: 10px; margin-top: 1px; display: inline-block; ">
    Liqiang Nie, Leigang Qu, Dai Meng, Min Zhang, Qi Tian, Alberto Del Bimbo.
    Search-oriented <br>Micro-video Captioning.
    ACM MM 2022 (oral). <a href="https://dl.acm.org/doi/10.1145/3503161.3548180">PDF</a>
</div>


<!-- Acknowledgements -->
<br><br>
<center>
    <h2><font face="helvetica" style="font-size:24px">Acknowledgements</font></h2>
    <hr style="margin-top:-10px; margin-bottom:13px">
</center>
<div style= "text-indent:25px; line-height: 1.5; text-align:justify">
    We sincerely thank the outstanding annotation team for their excellent work. 
    This work is partially supported by 
    the National Natural Science Foundation of China (No.:U1936203)
    and Kuaishou Inc. 

</div>



</tbody></table></body></html>


